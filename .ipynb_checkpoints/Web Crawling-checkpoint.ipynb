{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics of Crawling Sites and Scraping Web Pages\n",
    "\n",
    "The two most fundamental tools for anyone looking to crawl websites and/or scrape them for data are: \n",
    "- [Requests](http://docs.python-requests.org/en/master/) for making HTTP requests\n",
    "- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) for turning those requests into something useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick look at how these two packages work, let's start with the home page of my website on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://agbs2k8.github.io/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to use a custom request header that is part of what I'm going to send with my GET request for the page.  I found this on someone else's page years ago, and have used it for a long time.  I'm not sure where I even got it from at this point, but thank-you to whowever previously posted this somewhere on the web!\n",
    "\n",
    "This has helped me to not get caught by some of the anti-scraping / crawling measures I've encountered over the years.\n",
    "___\n",
    "## *Legal Disclaimer!*\n",
    "\n",
    "Please be aware of any applicable laws or such wherever you are, and wherever a site is hosted.  Generally, be sure to follow the site's ROBOTS.txt and more generally DON'T BE EVIL!  Remember, you can send tons of HTTP requests very quickly with tools like this, and you could cause noticeable impacts on someone's site.  You might look at some other references before going out and crawling someone else's website.  Ex: [Wikipedia](https://en.wikipedia.org/wiki/Web_crawler)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's out of the way, lets make our http request and store what we get back as ```r```.  IF you are not familiar with how the internet works (DNS, etc), you might want to do some research.  Thankfully, the Requests package does all of the heavy lifting for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers = hdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work?  You can check he status code and see what response you got.  If you are not familiar with HTTP response codes, you might look at [Wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a 200 response is \"OK\" and it means we should have gotten the page back from the server.  Is it something we can look at and have it make sense?  I like to check the encoding to see if it is something usable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "print(r.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode, perfect! We know how to deal with that. \n",
    "\n",
    "While I'm an ethical web-scraper, I do want to know how they might be tracking me.  Are they using cookies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "<class 'requests.cookies.RequestsCookieJar'>\n"
     ]
    }
   ],
   "source": [
    "print([x for x in r.cookies])\n",
    "print(type(r.cookies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cookie jar is empty (and that's an awsome name to use).  \n",
    "\n",
    "So what is in the request response anyways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE HTML>\\r\\n<!--\\r\\n\\tTEMPLATE: Story by HTML5 UP | html5up.net | @ajlkn | Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)\\r\\n\\tSITE CONTENT: \\xc2\\xa92018 AJ Wilson - All Rights Reserved\\r\\n-->\\r\\n<html>\\r\\n\\t<head>\\r\\n\\t\\t<!-- Google Tag Manager -->\\r\\n\\t\\t<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\r\\n\\t\\tnew Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\r\\n\\t\\tj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\r\\n\\t\\t\\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\r\\n\\t  })(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-NSXB5GC\\');</script>\\r\\n  \\t<!-- End Google Tag Manager -->\\r\\n\\t\\t<title>AJ Wilson</title>\\r\\n\\t\\t<meta charset=\"utf-8\" />\\r\\n\\t\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, user-scalable=no\" />\\r\\n\\t\\t<link rel=\"stylesheet\" href=\"assets/css/main.css\" />\\r\\n\\t\\t<noscript><link rel=\"stylesheet\" href=\"assets/css/noscript.css\" /></noscript>\\r\\n\\t</head>\\r\\n\\t<body class=\"is-preloa'\n"
     ]
    }
   ],
   "source": [
    "print(r.content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, that doesn't look friendly.  This is where BeautifulSoup comes in.  It will take this and turn it into a BeautifulSoup object that we can deal with more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.content,'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does our request resposne look like now? I'm going to call a function that we'll get to in a second to limit how much of the page we're going to pull.  I'll only pull the ```<head>``` tag because we don't need to see the entire page right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<!-- Google Tag Manager -->\n",
      "<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "\t\tnew Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "\t\tj=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "\t\t'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "\t  })(window,document,'script','dataLayer','GTM-NSXB5GC');</script>\n",
      "<!-- End Google Tag Manager -->\n",
      "<title>AJ Wilson</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1, user-scalable=no\" name=\"viewport\"/>\n",
      "<link href=\"assets/css/main.css\" rel=\"stylesheet\"/>\n",
      "<noscript><link href=\"assets/css/noscript.css\" rel=\"stylesheet\"/></noscript>\n",
      "</head>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('head')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better.  It now looks like what we would expect to see if we inspected the page with our browser's developer tools.  Now we have something that is ready to work with to find specific parts within the page.  \n",
    "\n",
    "BeautifulSoup comes with a fantastic set of tools for searching through HTML tags and finding specific pieces of a page.  I'm going to use the BeautifulSoup find_all() function to grab all ```<a>``` a / anchor tags, and I'll add the parameter of ```href=True``` to only select anchor tags that have an href defined.  I could use a regex rather than just using ```True```.  I'll pack all of this into a list comprehension (because they're awsome), filter out any link that doesn't point to an HTTP or HTTPS url (to remove my inter-page links). Then, put it into a ```set()``` to drop any duplicates and here we are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://anaconda.org/anaconda/python\n",
      "https://plot.ly\n",
      "https://www.javascript.com/\n",
      "https://en.wikipedia.org/wiki/Transact-SQL\n",
      "https://www.r-project.org/\n",
      "https://www.tensorflow.org\n",
      "http://www.numpy.org\n",
      "http://ggplot.yhathq.com/\n",
      "https://keras.io\n",
      "https://developer.salesforce.com/page/Apex\n",
      "https://d3js.org/\n",
      "http://hadoop.apache.org/\n",
      "https://en.wikipedia.org/wiki/Java_(programming_language)\n",
      "http://scikit-learn.org\n",
      "https://www.mongodb.com/\n",
      "https://www.raspberrypi.org/\n",
      "https://html5up.net\n",
      "https://github.com/agbs2k8/ML_Example_Notebooks/blob/master/Linear_Regression.ipynb\n",
      "https://seaborn.pydata.org\n",
      "https://www.python.org/\n",
      "https://pandas.pydata.org/\n",
      "https://www.sqlite.org\n",
      "https://neo4j.com/\n",
      "https://matplotlib.org\n",
      "https://www.postgresql.org/\n",
      "http://docs.python-requests.org\n",
      "https://www.scala-lang.org/\n"
     ]
    }
   ],
   "source": [
    "links = set([x['href'] for x in soup.find_all(\"a\", href=True) if x['href'][:4]=='http'])\n",
    "_=[print(x) for x in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! If I'm crawling a site, I now have a list of URLs to add to my list of pages to investigate.  \n",
    "\n",
    "This obviously isn't a perfect way to crawl a web page, however.  This is only showing me the links that were very intentionally placed there by the developers.  What about other links to the assets used (JS, CSS, Images)?  I know these things are not going to be found in a tag with this structure ```<a href='...'>``` and that was all I looked for. \n",
    "\n",
    "What other tags were out there that had a hypertext reference (href) in them? We can use the ```find_all()``` method from our BeautifulSoup object named ```soup```. I'll drop anything that starts with the ```#``` sign so that I drop the inner-page links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets/css/noscript.css\n",
      "https://anaconda.org/anaconda/python\n",
      "https://plot.ly\n",
      "images/gallery/fulls/04.jpg\n",
      "https://www.javascript.com/\n",
      "scikit-learn.org\n",
      "https://en.wikipedia.org/wiki/Transact-SQL\n",
      "images/gallery/fulls/02.jpg\n",
      "https://www.r-project.org/\n",
      "https://www.tensorflow.org\n",
      "http://www.numpy.org\n",
      "https://keras.io\n",
      "http://ggplot.yhathq.com/\n",
      "https://developer.salesforce.com/page/Apex\n",
      "https://d3js.org/\n",
      "http://hadoop.apache.org/\n",
      "https://en.wikipedia.org/wiki/Java_(programming_language)\n",
      "https://www.mongodb.com/\n",
      "assets/css/main.css\n",
      "http://docs.python-requests.org\n",
      "https://www.raspberrypi.org/\n",
      "mailto:agbs2k8@yahoo.com\n",
      "consulting.html\n",
      "https://html5up.net\n",
      "https://github.com/agbs2k8/ML_Example_Notebooks/blob/master/Linear_Regression.ipynb\n",
      "https://seaborn.pydata.org\n",
      "www.linkedin.com/in/agbs2k8\n",
      "https://www.python.org/\n",
      "https://pandas.pydata.org/\n",
      "https://www.sqlite.org\n",
      "images/gallery/fulls/03.jpg\n",
      "https://neo4j.com/\n",
      "https://matplotlib.org\n",
      "https://www.postgresql.org/\n",
      "http://scikit-learn.org\n",
      "https://www.scala-lang.org/\n"
     ]
    }
   ],
   "source": [
    "links = list(set([x['href'] for x in soup.find_all(href=True) if not str(x['href']).startswith('#')]))\n",
    "_=[print(x) for x in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we picked up a few more links. These all are either to other pages within my website, a ```mailto``` to contact me from the site, or links to the images and style-sheets that are part of the page when it renders in a browser.  This helps make my crawler much better, since now it will start identifying intra-site links and assets to build out a more complete crawl.  \n",
    "\n",
    "If I'm going to start crawling a web site, I'll need to come up with a way to parse and classify each of these URLs as I encounter them.  I believe it is important to know if a link is to another HTML page on the same site, to an asset, or to an outside site.  We could spend hours (or perhaps even days) building a way to parse our URLs, but why re-invent a wheel whe someone else has undoubtably done it for us. \n",
    "\n",
    "Meet [urllib](https://docs.python.org/3.6/library/urllib.html), part of the Python Standard Library, which has an excellent parsing function for URLs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='', netloc='', path='images/gallery/fulls/04.jpg', params='', query='', fragment='')\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "print(urlparse(links[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! It tells us as much data as we were likely to need in one simple call.  This can help our crawler understand what information is in the link itself prior to ever sending an HTTP request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A super-simple Web Crawler\n",
    "\n",
    "So now that we have seen how to use Requests & BeautifulSoup to make and parse the results of our GET requests, and use urllib to parse the links we find, we can put together a very simple function to crawl a website.  I've tried to clearly document everything going on in the function so it is easy to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "def crawl_site(start_from:str, multi_domain=False, page_limit=100) -> dict:\n",
    "    '''\n",
    "    A function to crawl websites, traversing any href in each page\n",
    "    :param start_from: string URI to begin crawling from\n",
    "    :param multi_domain: False (default) means stay in original domain, True = crawl any encountered domain\n",
    "    :param page_limit: integer max number of web pages to crawl\n",
    "    :return: a dictionary of pages crawled and their attributes\n",
    "    TODO: add depth limit - an integer folder depth to crawl through. will not crawl any folders deeper than this value\n",
    "    '''\n",
    "    # I'm going to pull in and use the same header as I introduced before\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    # make sure the passed paramenter is all lower case\n",
    "    start_from = str(start_from).lower()\n",
    "    # let's create a list of links I'm going to work from, starting with the original passed URI\n",
    "    links_to_crawl = [start_from]\n",
    "    # Check the domain we were given so we can limit our search to that domain\n",
    "    start_domain = urlparse(start_from).netloc\n",
    "    # Create a dictionary to store all of the information we pull as we crawl\n",
    "    results = {}\n",
    "    \n",
    "    # while loop because we don't know how many iterations we will ultimately do \n",
    "    # Limits are 1. if we run out of links, and 2. if we hit our page limit\n",
    "    while len(links_to_crawl) > 0 and len(results) <= page_limit:\n",
    "        # FIFO - pull links from the list of those I'm working on\n",
    "        current_url = links_to_crawl.pop(0)\n",
    "        # Create a temporary dictionary of results for this URI:\n",
    "        current_result = {}\n",
    "        # use urllib.parse.urlparse() to parse the parts of the latest URL\n",
    "        parsed_url = urlparse(current_url)\n",
    "        # Add the information out of the parsed URI to our results for this link\n",
    "        current_result.update({'scheme': parsed_url.scheme, 'domain': parsed_url.netloc, 'path': parsed_url.path})\n",
    "        \n",
    "        # request the page that we are lookin at\n",
    "        current_request = requests.get(current_url, headers = hdr)\n",
    "        soup = BeautifulSoup(current_request.content,'html.parser')\n",
    "        \n",
    "        # look at how many links were found on the page\n",
    "        links_found = [str(x['href']).lower() for x in soup.find_all(href=True) if not str(x['href']).startswith('#') ]\n",
    "        \n",
    "        # Add the data from that request to our site-result\n",
    "        current_result.update({'status': current_request.status_code, \n",
    "                               'encoding': current_request.encoding,\n",
    "                               'links_found':len(set(links_found))\n",
    "                              })\n",
    "        # Add the site-result to the master results dictionary    \n",
    "        results[current_url] = current_result\n",
    "        \n",
    "        # If it is a URL outside of the domain I'm working with, and we're only doing 1x domain do nothing else\n",
    "        if parsed_url.netloc != start_domain and not multi_domain:\n",
    "            continue\n",
    "        # otherwise, parse the page for additional links\n",
    "        else:\n",
    "            while len(links_found) > 0:\n",
    "                link = links_found.pop()\n",
    "                # standardize the link\n",
    "                new_link_parsed = urlparse(link)\n",
    "                # look for site-internal links without a netloc\n",
    "                \n",
    "                if new_link_parsed.netloc == '':\n",
    "                    link = urljoin(start_from, link) # use our starting link as the basis for it\n",
    "                \n",
    "                if new_link_parsed.scheme in (['http','https']) and link not in links_to_crawl and link not in results.keys():\n",
    "                    links_to_crawl.append(link)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and run the crawler and see what we got back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "crawl_results = crawl_site('https://agbs2k8.github.io', multi_domain=False, page_limit=50)\n",
    "print(len(crawl_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now also import the wonderful [Pandas](https://pandas.pydata.org/) package here so we can put the results into a nice table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>scheme</th>\n",
       "      <th>domain</th>\n",
       "      <th>path</th>\n",
       "      <th>status</th>\n",
       "      <th>encoding</th>\n",
       "      <th>links_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://docs.python-requests.org</td>\n",
       "      <td>http</td>\n",
       "      <td>docs.python-requests.org</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://ggplot.yhathq.com/</td>\n",
       "      <td>http</td>\n",
       "      <td>ggplot.yhathq.com</td>\n",
       "      <td>/</td>\n",
       "      <td>200</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://hadoop.apache.org/</td>\n",
       "      <td>http</td>\n",
       "      <td>hadoop.apache.org</td>\n",
       "      <td>/</td>\n",
       "      <td>200</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://scikit-learn.org</td>\n",
       "      <td>http</td>\n",
       "      <td>scikit-learn.org</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.numpy.org</td>\n",
       "      <td>http</td>\n",
       "      <td>www.numpy.org</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             index scheme                    domain path  \\\n",
       "0  http://docs.python-requests.org   http  docs.python-requests.org        \n",
       "1        http://ggplot.yhathq.com/   http         ggplot.yhathq.com    /   \n",
       "2        http://hadoop.apache.org/   http         hadoop.apache.org    /   \n",
       "3          http://scikit-learn.org   http          scikit-learn.org        \n",
       "4             http://www.numpy.org   http             www.numpy.org        \n",
       "\n",
       "   status    encoding  links_found  \n",
       "0     200  ISO-8859-1          175  \n",
       "1     200  ISO-8859-1           12  \n",
       "2     200  ISO-8859-1          112  \n",
       "3     200       utf-8            9  \n",
       "4     200       utf-8           23  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(crawl_results, orient='index').reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this came together really easily.  Python has some fantastic packages availble for web requests and handling HTML which makes this really easy.  \n",
    "\n",
    "I'll plan to touch on this again in the future, where I'll look at ways to speed this up and get it running faster.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
